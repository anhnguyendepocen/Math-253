---
title: "In-Class Computing Task 15"
author: "Math 253: Statistical Computing & Machine Learning"
date: "Tuesday Nov 3, 2015"
params:
  show_answers: FALSE
output: 
  rmarkdown::tufte_handout
---

```{r include=FALSE}
options(width=80)
library(mosaicData)
library(ggplot2)
library(scoreActivity)
library(ISLR)
knitr::opts_chunk$set(tidy=FALSE)
options(width=100)
```

Random matrix and it's principle components: cumsum of $d^2.

Covariance matrix of mona

Make X matrix.

Generating a sparse $\beta$.

Make Y response vector.

Use `lm()` for fit.  It's bang on.

Fitting with `pcr()` and `glmnet(alpha=1)`

Add a bit of noise to Y.  `lm()` degrades quickly.
`pcr()` and lasso aren't so quick.



Add a bit of noise

Evaluating the results.



```{r echo = FALSE}
# Set up the image
mona <- "/Users/kaplan/Dropbox/Courses-Fall-2015/Math-253/Daily-Programming/mona-bw.jpg"
img <- jpeg::readJPEG(mona)
mona <- t(sqrt(img[,,1]^2 + img[,,2]^2 + img[,,3]^2))
mona <- sqrt(mona)/max(mona[])
mona <- mona[,ncol(mona):1]
image(mona, col=gray((0:100)/100), xaxt="n", yaxt="n")
#save(mona, file="mona.rda")
```

# PLAN

1. Reconstruct mona with 1, 2, ..., 10 components
2. Generate random noise with the same covariance matrix as mona, plot it
3. Generate a sparse set of coefficients and the corresponding Y
4. How does pcr do?
5. How does lasso do?
6. 


```{r}
res <- svd(mona)
approx <- 0
for (i in 1:1) {
  approx <- approx + outer(res$u[,i], res$v[,i]) * res$d[i]
}
image(approx)
```


Compare the cummulative $d^2$ as a function of `ncomp`.  Much faster rise in the mona data.


Creating Random explanatory and response

```{r}
RR <- matrix(rnorm(191*250), nrow=250, ncol=191)
CM <- var(t(mona))
X <- RR %*% chol(CM)
```

```{r}
coefs <- cbind(sample(c(1,3,5,-4,-2, rep(0, 50)), size=191, replace = TRUE))
Y <- X %*% coefs + .2*rnorm(250)
```

```{r}
library(pls)
pcr.fit <- pcr(Y ~ X, scale = TRUE, validation="CV") 
```


About 20 components give a good MSEP

# Lasso

```{r}
library(glmnet)
lasso.mod <- glmnet(X, Y, alpha=1)
cv.out <- cv.glmnet(X, Y, alpha=1)
lasso.coef <- predict(cv.out, type = "coefficients", s = cv.out$lambda.min)
plot(lasso.coef[-1], jitter(coefs) ) # [-1] get rid of intercept
```

```{r}
mod <- lm(Y ~ X)
plot(coef(mod)[-1], jitter(coefs))
```
Many of the genuinely non-zero coefficients are at the extremes for lasso, but not for `lm()`



```{r echo=FALSE, eval=params$show_answers, results="asis"}
cat("# Test statments")
```

```{r echo=params$show_answers, eval=params$show_answers}
source("Day-14-Test-Statements.R", local = TRUE)
```

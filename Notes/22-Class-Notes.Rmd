---
title: "Class Notes"
author: "Statistical Computing & Machine Learning"
date: "Class 22"
output: rmarkdown::tufte_handout
---

```{r include=FALSE}
require(mosaic)
require(ISLR)
require(glmnet)
require(pls)
require(gam)
knitr::opts_chunk$set(tidy=FALSE)
```

# Review

The methods we have studied so far use *all* the data to fit the model.  But for classification, it may be that only the cases that are intermixed are telling for distinguishing one group from the other.  The bulk of cases, clustered happily in their home country, may not be so useful.

# Carbon markets and classifiers

[NPR story on China carbon markets](http://www.npr.org/sections/parallels/2015/11/24/457203851/china-plans-to-create-a-nationwide-carbon-market-by-20173)

1. Monopoly issuer of carbon permits. (Government)
2. Shutting down the permit violators has other social costs. 
    - Analogous to over-fitting. (Function *must* go through every point.)
    - They might be able to invest over time to reduce emission.
    - Allow "violations", but impose a cost (like square residual)
2. Permits can be bought and sold. You must buy enough permits to match your carbon production.
3. Some permit holders can easily reduce carbon emission in the short term (easy savers).  Others cannot (hidebound emitters).
5. Allow easy savers to sell their permits to hidebound emitters.


For our analogy, imagine that the government imposed requirements on both carbon *emitters* and carbon *absorbers*, e.g. the tree farm *must* be at least this big.



# Lines, planes, and hyperplanes

* $y = m x + b = \beta_0 + \beta_1 x$
* $z = a + b x + c y = \beta_0 + \beta_1 x + \beta_2 y = \beta_0 + \beta_1 x_1 + \beta_2 x_2$
    - $\beta_1$ and $\beta_2$ give the partial derivatives w.r.t. $x_1$ and $x_2$.
    - But what if the plane is intended to be parallel to the z-axis? Partials are infinite.
    - We would write the formula with $x_1$ or $x_2$ on the left-hand side.
    - $0 = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + beta_3 x_3$
    - $\beta_0$ sets the distance of the plane from the origin.
    - $\beta_1, \beta_2, ...$ set the orientation of the plane.

Another perspective. $d$ points in $d$-dimensional space define a hyperplane.  The hyperplane is a linear combination of the $d-1$ vectors $x_2 - x_1$, $x_3 - x_1$, $x_4 - x_1$, ...

The unique normal to the place is the vector not in the $d-1$ dimensional subspace.

![Plane](http://mathworld.wolfram.com/images/eps-gif/Plane_1001.gif)



## Rescaling X

It's the relative size of the $\beta_j$ that's important.  So let's agree to scale them so that $\sum_{j=1}^p \beta_j^2 = 1$.



## Impose an absolute constraint

Formalism:

* $y_i = 1$ means class A
* $y_i = -1$ means class B

Constraints:

$y_i ( \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_p x_{ip}) > 0$

\medskip

Says, "You must be on the right side of the boundary."

* Alternative:

$y_i ( \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_p x_{ip}) > M > 0$

\medskip

Says, "You must be on the right side of the boundary and at least $M$ units away from the boundary." Like, "You must produce $M$ units less than your permits allow, or less."

\includegraphics[width=\textwidth]{../ISL-Book-Figures/Chapter9/{9.2}.pdf}
\marginnote[-2cm]{Figure 9.2 from ISL}

There may be no *feasible solution*, or there may be many.  This formulation provides no way to deal with the no-solution situation, and no way to choose the best of the many solutions.

\includegraphics[width=\textwidth]{../ISL-Book-Figures/Chapter9/{9.3}.pdf}
\marginnote[-2cm]{Figure 9.3 from ISL}

# Optimizing within the constraint

* maximize over $\beta_0$, $\beta_1$, ... the quantity $M$
* constraints that $y_i ( \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_p x_{ip}) > M$
* and remember that $\sum_{j=1}^{p} \beta_j^2 = 1$.

Says, "You must be on the right side of the boundary and as far from it as possible." $M$ measures how far from boundary.





\includegraphics[width=\textwidth]{../ISL-Book-Figures/Chapter9/{9.5}.pdf}
\marginnote[-2cm]{Figure 9.5 from ISL}

## Allowing violations of the boundary

* maximize over $\beta_0$, $\beta_1$, ... the quantity $M$
* constraints that $y_i ( \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_p x_{ip}) > M (1 - \epsilon_i)$
* $\epsilon_i \geq 0$ --- Once you're on the good side of the margin, you don't get any credit that could be used to increase $M$!
* $\sum_{i=1}^n \epsilon_i < C$ --- but let's keep the epsilons small.  We're willing to trade off between epsilons.  
    - $C$ is the budget for violations, a *tuning parameter*.
    - For a given $C$, there's no guarantee that we can meet the constraints. (That is, no guarantee that there's a *feasible solution*.)
    - But there will be some $C$ for which there is a feasible solution.

* and remember that $\sum_{j=1}^{p} \beta_j^2 = 1$.


\includegraphics[width=\textwidth]{../ISL-Book-Figures/Chapter9/{9.6}.pdf}
\marginnote[-2cm]{Figure 9.6 from ISL}

FIGURE 9.6. Left: A support vector classifier was fit to a small data set. The hyperplane is shown as a solid line and the margins are shown as dashed lines. Purple observations: Observations 3,4,5, and 6 are on the correct side of the margin, observation 2 is on the margin, and observation 1 is on the wrong side of the margin. Blue observations: Observations 7 and 10 are on the correct side of the margin, observation 9 is on the margin, and observation 8 is on the wrong side of the margin. No observations are on the wrong side of the hyperplane. Right: Same as left panel with two additional points, 11 and 12. These two observations are on the wrong side of the hyperplane and the wrong side of the margin.


\includegraphics[width=\textwidth]{../ISL-Book-Figures/Chapter9/{9.7}.pdf}
\marginnote[-2cm]{Figure 9.7 from ISL}

FIGURE 9.7. A support vector classifier was fit using four different values of the tuning parameter $C$ in (9.12)â€“(9.15). The largest value of $C$ was used in the top left panel, and smaller values were used in the top right, bottom left, and bottom right panels. When $C$ is large, then there is a high tolerance for observations being on the wrong side of the margin, and so the margin will be large. As $C$ decreases, the tolerance for observations being on the wrong side of the margin decreases, and the margin narrows.

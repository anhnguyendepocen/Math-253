---
title: "Class Notes"
author: "Statistical Computing & Machine Learning"
date: "Class 14"
output: rmarkdown::tufte_handout
---

```{r include=FALSE}
require(mosaic)
require(ISLR)
knitr::opts_chunk$set(tidy=FALSE)
```



# Model & Theory building

## Occam's Razor: A heuristic

*Non sunt multiplicanda entia sine necessitate*.

* Entities must not be multiplied beyond necessity.

But what's "beyond necessity?"

## Einstein's proverb

*Man muß die Dinge so einfach wie möglich machen. Aber nicht einfacher*.

* Make things as simple as possible, but not one bit simpler.

But what's "too simple?"

# Statistical parallel

## Bias versus variance

Bias refers to the ways that $\hat{f}({\mathbf X})$ is systematically different from the idealized $f({\mathbf X})$.

Variance refers to the ways that $\hat{f}_i({\mathbf X})$ varies with the particular sample $i$ used for training.

## Goal for model building

**Make model error small**

**Sub-Goals**:

* Make bias as small as possible
    - use many parameters, flexible function architectures.
* Make variance as small as possible.
    - use large training sample size $n$
    - use few parameters/stiff function architectures
    
Basic modeling-building question:    
- Does adding this [variable, model term, potential flexibility] help?
    - For reducing bias: yes
    - For reducing variance: maybe (if it eats up residual variance)
    - For reducing error: maybe
    
## Operationalizing model choice

Think in terms of comparing two models to select which one is better.

"Better" needs to be transitive: if A is better than B, and B is better than C, then A is better than C.

Then, any number of models can be compared to find the one (or more) that is the best.

## Some definitions of "better"

* Larger likelihood (non-iid Gaussian error models)
* Smaller mean square prediction error (same as larger likelihood with iid Gaussian error model)
* Classification error rate is smaller
* Loss function is smaller

## Training and Testing

Evaluation of performance using training data is biased to give larger likelihood (smaller MSE or classification error or loss error).

Unbiased evaluation is done on separate testing data.

## Trade-off

* Need large testing dataset for good estimate of performance

* Need large training dataset for reducing variance in fit.

How to get both:

1. College a huge amount of data.  When this works, go for it!
2. K-fold cross validation
    - Pull out 1/K part of the data for performance testing.
    - Fit to the other (K-1)/K part of the data.
    - Repeat K times and average the prediction results over the K trials.
3. Once you've found the best *form* of model, fit it to the whole data set.  That's your model.

# Programming Basics: Loops/Iteration

Loops are the programming control structure that allows you to repeat the same commands many times.

*A definition of insanity*: Doing something over and over again and expecting a different result.




# Jubilee Week

Work on completing the first 12 in-class activities.

